# -*- coding: utf-8 -*-
"""Breca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pWCiIDM0FkJxvK9aIr_5uDfibpK09gvc
"""

import sys
import os
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras import optimizers
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation
from tensorflow.python.keras.layers import  Convolution2D, MaxPooling2D
from tensorflow.python.keras import backend as K

"""Limpiamos la sesión y agregamos todos los archivos presentes en nuestro gitHub, así tendremos disponible el dataset y códigos dentro del Colaboratory."""

K.clear_session()

! git clone https://github.com/ReynaldoSQ/BreCa.git

"""Una vez que ya tenemos los archivos, establecemos las rutas para acceder tanto a los datos de entrenamiento como a los de validación, si damos clic en la flecha a la izquierda del portal, podemos ver los archivos y el tree de contenido, copiamos la ruta de la carpeta y las agregamos como texto en cada variable."""

data_entrenamiento = './BreCa/Entrenamiento'
data_validacion = './BreCa/Validacion'

"""Si deseamos ver una imagen que se encuentre dentro de nuestro conjunto de imagenes sustituimos la dirección de la variable imagen_t"""

import matplotlib.pyplot as plt
imagen_t=plt.imread('./BreCa/Entrenamiento/Ductal/1.JPG')
plt.imshow(imagen_t)

imagen_t=plt.imread('./BreCa/Entrenamiento/Lobular/a.JPG')
plt.imshow(imagen_t)

"""Definimos las variables necesarias para construir nuestra red neuronal convolucional.
Datos importantes a considerar:
Epocas= epocas de entrenamiento
Longitud altura= son las dimensiones a las que reescalaremos nuestras imágenes para que queden normalizadas.
Batch_size= lote de entreamiento
número de pasos para reconocer en la imagen (en píxeles)
número de filtros y tamaño de cada filtro
Las clases que vamos a identificar (Número de carpetas en nuestro dataset de entrenamiento)
"""

epocas=20 
longitud, altura = 50, 50
batch_size = 10
pasos = 20
validation_steps = 15
filtrosConv1 = 32
filtrosConv2 = 64
tamano_filtro1 = (3, 3)
tamano_filtro2 = (2, 2)
tamano_pool = (2, 2)
clases = 2
lr = 0.0004

"""'''procesamiento de imágenes, reescalar , deformar y aumentar''"""

entrenamiento_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

"""Se asigan los parámetros para construir el esquema de entrenamiento"""

entrenamiento_generador = entrenamiento_datagen.flow_from_directory(
    data_entrenamiento,
    target_size=(altura, longitud),
    batch_size=batch_size,
    class_mode='categorical')

validacion_generador = test_datagen.flow_from_directory(
    data_validacion,
    target_size=(altura, longitud),
    batch_size=batch_size,
    class_mode='categorical')

cnn = Sequential()
cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, padding ="same", input_shape=(longitud, altura, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=tamano_pool))
cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding ="same"))
cnn.add(MaxPooling2D(pool_size=tamano_pool))

cnn.add(Flatten())
cnn.add(Dense(256, activation='relu'))
cnn.add(Dropout(0.5))
cnn.add(Dense(clases, activation='softmax'))

cnn.compile(loss='categorical_crossentropy',
            optimizer=optimizers.Adam(lr=lr),
            metrics=['accuracy'])

"""Iniciar el entrenamiento"""

cnn.fit_generator(
    entrenamiento_generador,
    steps_per_epoch=pasos,
    epochs=epocas,
    validation_data=validacion_generador,
    validation_steps=validation_steps)

imagen=plt.imread('./BreCa/Entrenamiento/Ductal/1.JPG')
cnn.predict(imagen.reshape(1,50,50))

"""Almacenar el modelo obtenido a partir del set de datos y esquema de entrenamiento definido anteriormente"""

target_dir = './modelo/'
if not os.path.exists(target_dir):
  os.mkdir(target_dir)
cnn.save('./modelo/modelo.h5')
cnn.save_weights('./modelo/pesos.h5')
